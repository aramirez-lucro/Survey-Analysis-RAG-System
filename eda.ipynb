{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sustainability_path = \"backend/data/Dataset 1 (Sustainability Research Results).xlsx\"\n",
    "\n",
    "sustainability_df = pd.read_excel(sustainability_path, engine='openpyxl')\n",
    "\n",
    "sustainability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_blocks(df):\n",
    "    # Identify the rows where questions start\n",
    "    question_rows = df[df.iloc[:, 0].str.contains('Question', na=False)].index\n",
    "\n",
    "    # Create a dictionary to store the split dataframes\n",
    "    dfs = {}\n",
    "\n",
    "    # Iterate over the question rows and split the dataframe\n",
    "    for i, start_row in enumerate(question_rows):\n",
    "        end_row = question_rows[i + 1] if i + 1 < len(question_rows) else len(df)\n",
    "        question_text = df.iloc[start_row, 0]\n",
    "        dfs[question_text] = df.iloc[start_row:end_row].reset_index(drop=True)#.fillna('')\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustainability_dfs = extract_data_blocks(sustainability_df)\n",
    "\n",
    "len(sustainability_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to excel files\n",
    "for i, (key, df) in enumerate(sustainability_dfs.items()):\n",
    "    key = \"_\".join(key.split(\" \")[:2]).lower()\n",
    "    df.to_excel(f\"backend/data/processed/sustainability_{key}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_path = \"backend/data/Dataset 2 (Christmas Research Results).xlsx\"\n",
    "christmas_df = pd.read_excel(christmas_path, engine='openpyxl')\n",
    "\n",
    "christmas_dfs = extract_data_blocks(christmas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (key, df) in enumerate(christmas_dfs.items()):\n",
    "    key = \"_\".join(key.split(\" \")[:2]).lower()\n",
    "    df.to_excel(f\"backend/data/processed/christmas_{key}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the SurveyAnalysisRAGSystem\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from backend.utils.logging import get_logger\n",
    "from time import time\n",
    "from glob import glob\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Set your OpenAI API Key\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "class DocumentLoader:\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def load_documents(self):\n",
    "        \"\"\"Load and split the documents from the Excel files using the UnstructuredExcelLoader.\"\"\"\n",
    "        logger.info(f\"Loading documents from {len(self.file_paths)} files: {self.file_paths}\")\n",
    "        docs = [self._process(file_path) for file_path in self.file_paths]\n",
    "        documents = sum(docs, [])  # Flatten the list of lists\n",
    "        logger.info(f\"Total documents loaded: {len(documents)}\")\n",
    "        return documents\n",
    "\n",
    "    def _process(self, file_path):\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "        docs = UnstructuredExcelLoader(file_path, mode='elements').load_and_split()\n",
    "        for doc in docs:\n",
    "            dataset_id = file_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "            doc.metadata['dataset_id'] = dataset_id\n",
    "        logger.info(f\"Loaded {len(docs)} documents from {file_path}\")\n",
    "        return docs\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, documents, embedding_model):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        \"\"\"Generate embeddings for the documents and create a FAISS vectorstore for retrieval.\"\"\"\n",
    "        logger.info(\"Generating embeddings for documents\")\n",
    "        embeddings = OpenAIEmbeddings(model=self.embedding_model)\n",
    "        vectorstore = FAISS.from_documents(self.documents, embeddings)\n",
    "        logger.info(f\"Vectorstore created successfully with {len(self.documents)} documents\")\n",
    "        return vectorstore\n",
    "\n",
    "class PromptTemplateFactory:\n",
    "    @staticmethod\n",
    "    def create_prompt_template():\n",
    "        \"\"\"Define the prompt template for querying the model with relevant context.\"\"\"\n",
    "        logger.info(\"Defining prompt template\")\n",
    "        return ChatPromptTemplate(\n",
    "            input_variables=['context', 'question'],\n",
    "            messages=[\n",
    "                HumanMessagePromptTemplate(\n",
    "                    prompt=PromptTemplate(\n",
    "                        input_variables=['context', 'question'],\n",
    "                        template=(\n",
    "                            \"You are an assistant for question-answering tasks. \"\n",
    "                            \"Specialized in data analysis and interpreting survey data. \"\n",
    "                            \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                            \"If you don't know the answer, just say that you don't know. \"\n",
    "                            \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "                            \"Question: {question} \\n\"\n",
    "                            \"Context: {context} \\n\"\n",
    "                            \"Answer:\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "class SurveyAnalysisRAGSystem:\n",
    "    def __init__(self, file_path, embedding_model=\"text-embedding-3-large\", llm_model=\"gpt-4o-mini\"):\n",
    "        logger.info(\"Initializing SurveyAnalysisRAGSystem\")\n",
    "        self.file_paths = glob(file_path + \"*.xlsx\")\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Load documents\n",
    "        self.documents = DocumentLoader(self.file_paths).load_documents()\n",
    "\n",
    "        # Create vector embeddings and store them in FAISS\n",
    "        self.vectorstore = VectorStore(self.documents, self.embedding_model).create_vectorstore()\n",
    "\n",
    "        # Initialize the language model for generating insights\n",
    "        self.llm = ChatOpenAI(model=self.llm_model, temperature=0.25, max_tokens=1024)\n",
    "\n",
    "        # Setup prompt template for query generation\n",
    "        self.prompt = PromptTemplateFactory.create_prompt_template()\n",
    "\n",
    "    @staticmethod\n",
    "    def format_docs(docs):    \n",
    "        return \"\\n\\n\".join(str(doc.metadata[\"filename\"]) + \"\\n\\n\" + doc.page_content for doc in docs) \n",
    "\n",
    "    def generate_answer(self, query, dataset_id, k=16):\n",
    "        \"\"\"Generate an answer for the given query using the specified dataset.\"\"\"\n",
    "        logger.info(f\"Generating answer for query: {query} with dataset_id: {dataset_id}\")\n",
    "        st = time()\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\", # mmr\n",
    "            search_kwargs={'k': k, 'lambda_mult': 0.25, 'filter': {'dataset_id': dataset_id}}\n",
    "        )\n",
    "        self.qa_chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | self.format_docs,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Invoking QA chain...\")\n",
    "        output = {\"result\" : self.qa_chain.invoke(query)}\n",
    "        output[\"time_taken\"] = time() - st\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your file paths\n",
    "file_path = 'backend/data/processed/'\n",
    "\n",
    "# Initialize the system with the file paths\n",
    "logger.info(\"Initializing SurveyAnalysisRAGSystem with example file paths\")\n",
    "survey_system = SurveyAnalysisRAGSystem(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the system with a specific question\n",
    "query = \"summarize results for question 1\"\n",
    "logger.info(f\"Querying system with: {query}\")\n",
    "\n",
    "response = survey_system.generate_answer(query, dataset_id='sustainability', k=16)\n",
    "\n",
    "# Print the response\n",
    "logger.info(f\"Response: {response}\")\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
